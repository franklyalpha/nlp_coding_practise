"""
Defining reward model:

start with an abstract class defining all the properties the reward model must include, and
need to write specifics for how language models should interact with it. Remember to handle the
devices properly.

Reward model should perform the following procedure:
After receiving the output of language model, it will use the network model to evaluate
the output, and gives a reward signal. The reward signal will then be used to train the
model. May consider having one method for providing the reward signal, and another
general method for fine-tuning the language model.

one problem is how should the fine tuning being conducted. Realizing in typical RL, a
trajectory needs to be sampled before training could go through (delayed reward).
Is this the same case for language model fine-tuning?

"""
import torch
import torch.nn as nn
import torchtext as ttext
import numpy as np


class AbstractRewardModel(nn.Module):
    def __init__(self):
        super(AbstractRewardModel, self).__init__()

    def _init_network(self):
        pass

    def forward(self, input_prompt, response_initial, response_tuned):
        """


        :param input_prompt: the prompt used for language generative model
            to output a result.
        :param response_initial: the response generated by pretrained model
        :param response_tuned: the response generated by current model, fine tuned.

        All above parameters should have shape [sequence_length, batch_size, model_size], where each element
        is a numeric index that corresponds to a token in the Vocab object.
            model_size is the output shape of the language generative model, which is usually 512.

        :return: a scalar score as reward signal, calculated following RLHF's paper,
            using KL divergence of scores between two responses.

        """
        assert input_prompt.shape[1] == response_initial.shape[1] \
            and input_prompt.shape[1] == response_tuned.shape[1]

        return  # just a random number


    def fine_tune(self):
        pass

    def _fine_tune_one_observation(self):
        """

        :return:
        """
        # the key is to make the weight of transformer trainable, so loss
        # could be calculated along the way.

